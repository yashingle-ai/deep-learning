{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekVzSW8Bb_Ze",
        "outputId": "70b88995-3d42-4e12-971e-d1a9fbf5657d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "import wandb\n",
        "import os"
      ],
      "metadata": {
        "id": "HUjjD6MRYJmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Initialize wandb\n",
        "\n",
        "wandb.init(project=\"VGG-flowers-v3\", config={\n",
        "    \"epochs\": 50,\n",
        "    \"batch_size\": 16,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": \"VGG\",\n",
        "    \"pretrained\": True,\n",
        "    \"input_size\": 224\n",
        "})\n",
        "\n",
        "# Shortcut to config values\n",
        "config = wandb.config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "zxVl69yfYLO3",
        "outputId": "a142ce20-4e44-46c2-c02a-c2393377bfda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvizuara-info\u001b[0m (\u001b[33mpritkudale-vizuara\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250612_085526-4vnjskdt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/pritkudale-vizuara/Inception-flowers/runs/4vnjskdt' target=\"_blank\">sandy-dream-1</a></strong> to <a href='https://wandb.ai/pritkudale-vizuara/Inception-flowers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/pritkudale-vizuara/Inception-flowers' target=\"_blank\">https://wandb.ai/pritkudale-vizuara/Inception-flowers</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/pritkudale-vizuara/Inception-flowers/runs/4vnjskdt' target=\"_blank\">https://wandb.ai/pritkudale-vizuara/Inception-flowers/runs/4vnjskdt</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# STEP 1: Data Preparation\n",
        "# =======================\n",
        "\n",
        "# Transforms for training and validation\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/5flowersdata/flowers/train\"\n",
        "val_dir = \"/content/drive/MyDrive/5flowersdata/flowers/val\"\n",
        "\n",
        "train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms['train'])\n",
        "val_dataset = datasets.ImageFolder(root=val_dir, transform=data_transforms['val'])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config.batch_size)"
      ],
      "metadata": {
        "id": "Ti4rs4tkYMos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# STEP 2: Load Pretrained Model\n",
        "# ===========================\n",
        "\n",
        "model = models.vgg16(pretrained=config.pretrained)\n",
        "\n",
        "# Freeze feature layers\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace classifier\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Linear(25088, 4096),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096, 4096),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(),\n",
        "    nn.Linear(4096, 5)  # 5 output classes for flower dataset\n",
        ")\n",
        "\n",
        "# Only unfreeze the final classifier layer (index -1)\n",
        "for param in model.classifier[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Move model to device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Watch the model's weights and gradients\n",
        "wandb.watch(model, log=\"all\", log_freq=10)"
      ],
      "metadata": {
        "id": "Uu1_CDgvYOEW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f47d6cf-335f-44b7-db00-a6aa74e66698"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n",
            "100%|██████████| 49.7M/49.7M [00:00<00:00, 154MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# STEP 3: Loss & Optimizer\n",
        "# ===================\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)"
      ],
      "metadata": {
        "id": "mxiVbXfxYPiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g-opvZQESck"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=10):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            batch_correct = (preds == labels).sum().item()\n",
        "            train_correct += batch_correct\n",
        "            train_total += labels.size(0)\n",
        "\n",
        "            # Print every 10 batches\n",
        "            if (i + 1) % 10 == 0:\n",
        "                batch_acc = batch_correct / labels.size(0)\n",
        "                print(f\"[Batch {i+1}/{len(train_loader)}] Loss: {loss.item():.4f}, Batch Acc: {batch_acc:.4f}\")\n",
        "\n",
        "        train_acc = train_correct / train_total\n",
        "        wandb.log({\"epoch\": epoch + 1, \"train_loss\": running_loss, \"train_accuracy\": train_acc})\n",
        "        print(f\"Epoch {epoch+1} Summary - Loss: {running_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_correct += (preds == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        wandb.log({\"epoch\": epoch + 1, \"val_accuracy\": val_acc})\n",
        "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# Train the model\n",
        "# ===================\n",
        "train_model(model, criterion, optimizer, train_loader, val_loader, epochs=config.epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTczSawNYSrI",
        "outputId": "1abb990d-4e78-4743-b093-9b91dcac9b4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 1.5526, Batch Acc: 0.2500\n",
            "[Batch 20/251] Loss: 1.1847, Batch Acc: 0.6875\n",
            "[Batch 30/251] Loss: 1.1782, Batch Acc: 0.5625\n",
            "[Batch 40/251] Loss: 1.2513, Batch Acc: 0.4375\n",
            "[Batch 50/251] Loss: 1.1144, Batch Acc: 0.6250\n",
            "[Batch 60/251] Loss: 1.0016, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.8607, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.7827, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 1.0105, Batch Acc: 0.7500\n",
            "[Batch 100/251] Loss: 0.9265, Batch Acc: 0.6250\n",
            "[Batch 110/251] Loss: 0.8028, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.8574, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.7664, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.9041, Batch Acc: 0.6875\n",
            "[Batch 150/251] Loss: 0.6926, Batch Acc: 0.6250\n",
            "[Batch 160/251] Loss: 0.7376, Batch Acc: 0.7500\n",
            "[Batch 170/251] Loss: 0.5677, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.6926, Batch Acc: 0.7500\n",
            "[Batch 190/251] Loss: 0.7766, Batch Acc: 0.6875\n",
            "[Batch 200/251] Loss: 0.6578, Batch Acc: 0.6250\n",
            "[Batch 210/251] Loss: 0.5225, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.5256, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.5269, Batch Acc: 1.0000\n",
            "[Batch 240/251] Loss: 0.4769, Batch Acc: 1.0000\n",
            "[Batch 250/251] Loss: 0.5200, Batch Acc: 0.9375\n",
            "Epoch 1 Summary - Loss: 223.8322, Train Accuracy: 0.7043\n",
            "Validation Accuracy: 0.8526\n",
            "\n",
            "Epoch 2/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.5784, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.7324, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.4252, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.7094, Batch Acc: 0.7500\n",
            "[Batch 50/251] Loss: 0.6700, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.2817, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.6700, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.6827, Batch Acc: 0.6250\n",
            "[Batch 90/251] Loss: 0.3686, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.5793, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.3897, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.6099, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.6831, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 1.0052, Batch Acc: 0.6875\n",
            "[Batch 150/251] Loss: 0.5147, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.4303, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.5161, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.9612, Batch Acc: 0.5000\n",
            "[Batch 190/251] Loss: 0.2959, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.5021, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.5354, Batch Acc: 0.6875\n",
            "[Batch 220/251] Loss: 0.8400, Batch Acc: 0.7500\n",
            "[Batch 230/251] Loss: 0.2974, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.4951, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.3131, Batch Acc: 0.8750\n",
            "Epoch 2 Summary - Loss: 146.1974, Train Accuracy: 0.8074\n",
            "Validation Accuracy: 0.8427\n",
            "\n",
            "Epoch 3/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.5967, Batch Acc: 0.6875\n",
            "[Batch 20/251] Loss: 0.8666, Batch Acc: 0.6875\n",
            "[Batch 30/251] Loss: 0.7678, Batch Acc: 0.7500\n",
            "[Batch 40/251] Loss: 0.3720, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.7550, Batch Acc: 0.6875\n",
            "[Batch 60/251] Loss: 0.4038, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.3951, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.3676, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.2404, Batch Acc: 1.0000\n",
            "[Batch 100/251] Loss: 0.5236, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.6610, Batch Acc: 0.6875\n",
            "[Batch 120/251] Loss: 0.6519, Batch Acc: 0.6875\n",
            "[Batch 130/251] Loss: 0.5937, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.3895, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.4374, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.2833, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.6176, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.5535, Batch Acc: 0.6875\n",
            "[Batch 190/251] Loss: 0.4828, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.3412, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.4716, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.3559, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.4026, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.3848, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.4089, Batch Acc: 0.8750\n",
            "Epoch 3 Summary - Loss: 133.7464, Train Accuracy: 0.8124\n",
            "Validation Accuracy: 0.8704\n",
            "\n",
            "Epoch 4/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4056, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.7965, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.5122, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.6886, Batch Acc: 0.7500\n",
            "[Batch 50/251] Loss: 0.5065, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.3328, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.4078, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.4274, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.2879, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.3804, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.4982, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.7313, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.3048, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.4000, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.3669, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.2647, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.5579, Batch Acc: 0.7500\n",
            "[Batch 180/251] Loss: 0.9077, Batch Acc: 0.6875\n",
            "[Batch 190/251] Loss: 0.5072, Batch Acc: 0.7500\n",
            "[Batch 200/251] Loss: 0.1507, Batch Acc: 1.0000\n",
            "[Batch 210/251] Loss: 0.2098, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.3673, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.5914, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.3031, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.9561, Batch Acc: 0.4375\n",
            "Epoch 4 Summary - Loss: 121.8589, Train Accuracy: 0.8288\n",
            "Validation Accuracy: 0.8586\n",
            "\n",
            "Epoch 5/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4538, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.4651, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.3965, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.2266, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.9924, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.4636, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.5864, Batch Acc: 0.7500\n",
            "[Batch 80/251] Loss: 0.3432, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.7046, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.2521, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.1531, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.5290, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.2042, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.5668, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.3386, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.5311, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.9442, Batch Acc: 0.6875\n",
            "[Batch 180/251] Loss: 0.2582, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.4595, Batch Acc: 0.8125\n",
            "[Batch 200/251] Loss: 0.2149, Batch Acc: 1.0000\n",
            "[Batch 210/251] Loss: 0.3849, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.3040, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.3727, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 1.0968, Batch Acc: 0.5625\n",
            "[Batch 250/251] Loss: 0.4689, Batch Acc: 0.8125\n",
            "Epoch 5 Summary - Loss: 118.4905, Train Accuracy: 0.8293\n",
            "Validation Accuracy: 0.8625\n",
            "\n",
            "Epoch 6/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2397, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.3210, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.3063, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.6736, Batch Acc: 0.6875\n",
            "[Batch 50/251] Loss: 0.2584, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.4014, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.5685, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.4950, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.4299, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.7654, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.3648, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.1663, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.4078, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.8750, Batch Acc: 0.6250\n",
            "[Batch 150/251] Loss: 0.2877, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.4354, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.6896, Batch Acc: 0.6875\n",
            "[Batch 180/251] Loss: 0.4582, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.5972, Batch Acc: 0.8125\n",
            "[Batch 200/251] Loss: 0.5135, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.4067, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.1851, Batch Acc: 1.0000\n",
            "[Batch 230/251] Loss: 0.3055, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.8421, Batch Acc: 0.6875\n",
            "[Batch 250/251] Loss: 0.7874, Batch Acc: 0.6250\n",
            "Epoch 6 Summary - Loss: 115.4699, Train Accuracy: 0.8278\n",
            "Validation Accuracy: 0.8665\n",
            "\n",
            "Epoch 7/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.7625, Batch Acc: 0.6875\n",
            "[Batch 20/251] Loss: 0.5721, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.4809, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.3537, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.5185, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.5945, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.5675, Batch Acc: 0.6250\n",
            "[Batch 80/251] Loss: 0.3353, Batch Acc: 0.7500\n",
            "[Batch 90/251] Loss: 0.6882, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.3185, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.5078, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.3427, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.7962, Batch Acc: 0.6875\n",
            "[Batch 140/251] Loss: 0.9163, Batch Acc: 0.5000\n",
            "[Batch 150/251] Loss: 0.4739, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.8018, Batch Acc: 0.7500\n",
            "[Batch 170/251] Loss: 0.4077, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.3902, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.4855, Batch Acc: 0.8125\n",
            "[Batch 200/251] Loss: 0.6446, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.2812, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 1.1170, Batch Acc: 0.5625\n",
            "[Batch 230/251] Loss: 0.4377, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.4662, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.2629, Batch Acc: 0.8750\n",
            "Epoch 7 Summary - Loss: 113.8095, Train Accuracy: 0.8318\n",
            "Validation Accuracy: 0.8734\n",
            "\n",
            "Epoch 8/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4788, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.6088, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.5251, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.2923, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.1919, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.5783, Batch Acc: 0.6875\n",
            "[Batch 70/251] Loss: 0.2916, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.1568, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.1915, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.4189, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.6778, Batch Acc: 0.7500\n",
            "[Batch 120/251] Loss: 0.4940, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.9190, Batch Acc: 0.6875\n",
            "[Batch 140/251] Loss: 0.3254, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.1761, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.1699, Batch Acc: 1.0000\n",
            "[Batch 170/251] Loss: 0.3133, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.4697, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.3713, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.4381, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.1914, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.2058, Batch Acc: 1.0000\n",
            "[Batch 230/251] Loss: 0.7566, Batch Acc: 0.7500\n",
            "[Batch 240/251] Loss: 0.2415, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.5850, Batch Acc: 0.6250\n",
            "Epoch 8 Summary - Loss: 110.8167, Train Accuracy: 0.8378\n",
            "Validation Accuracy: 0.8724\n",
            "\n",
            "Epoch 9/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4533, Batch Acc: 0.7500\n",
            "[Batch 20/251] Loss: 0.4017, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.3800, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.4795, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.3956, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.6475, Batch Acc: 0.6875\n",
            "[Batch 70/251] Loss: 0.4890, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.1945, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.4701, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.7143, Batch Acc: 0.6250\n",
            "[Batch 110/251] Loss: 0.1422, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 1.0658, Batch Acc: 0.6250\n",
            "[Batch 130/251] Loss: 0.5644, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.4224, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.4583, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.3091, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.3100, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.3860, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.6278, Batch Acc: 0.6875\n",
            "[Batch 200/251] Loss: 0.1474, Batch Acc: 1.0000\n",
            "[Batch 210/251] Loss: 0.6181, Batch Acc: 0.6875\n",
            "[Batch 220/251] Loss: 0.5112, Batch Acc: 0.7500\n",
            "[Batch 230/251] Loss: 0.4168, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.3319, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.4099, Batch Acc: 0.8125\n",
            "Epoch 9 Summary - Loss: 105.3459, Train Accuracy: 0.8441\n",
            "Validation Accuracy: 0.8556\n",
            "\n",
            "Epoch 10/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4908, Batch Acc: 0.7500\n",
            "[Batch 20/251] Loss: 0.5643, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.3034, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.1671, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.1896, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.5296, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.6968, Batch Acc: 0.7500\n",
            "[Batch 80/251] Loss: 0.2301, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.7048, Batch Acc: 0.6875\n",
            "[Batch 100/251] Loss: 0.3784, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.3988, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.6424, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.5679, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.2372, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.6111, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.3256, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.4278, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.6961, Batch Acc: 0.6250\n",
            "[Batch 190/251] Loss: 0.3030, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.4483, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.3749, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.5341, Batch Acc: 0.7500\n",
            "[Batch 230/251] Loss: 0.2935, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.4948, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.2249, Batch Acc: 0.9375\n",
            "Epoch 10 Summary - Loss: 107.7832, Train Accuracy: 0.8406\n",
            "Validation Accuracy: 0.8793\n",
            "\n",
            "Epoch 11/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.3708, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.3865, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.3097, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.2380, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.4059, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.0891, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.3926, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.5589, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.4497, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.1550, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.3985, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.5099, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.5112, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.2688, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.5607, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.7675, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.1342, Batch Acc: 1.0000\n",
            "[Batch 180/251] Loss: 0.4090, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.3197, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 1.0148, Batch Acc: 0.6875\n",
            "[Batch 210/251] Loss: 0.2992, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.1865, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.4533, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.3042, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.3644, Batch Acc: 0.8125\n",
            "Epoch 11 Summary - Loss: 99.4047, Train Accuracy: 0.8553\n",
            "Validation Accuracy: 0.8843\n",
            "\n",
            "Epoch 12/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2528, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.3376, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.1696, Batch Acc: 1.0000\n",
            "[Batch 40/251] Loss: 0.7632, Batch Acc: 0.6250\n",
            "[Batch 50/251] Loss: 0.2908, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.4202, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.1621, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.3754, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.3643, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.5256, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.2367, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.6510, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.4235, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.4493, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.4762, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.3779, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.5724, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.1934, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.7169, Batch Acc: 0.7500\n",
            "[Batch 200/251] Loss: 0.5374, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.3075, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.4008, Batch Acc: 0.6875\n",
            "[Batch 230/251] Loss: 0.3327, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.3519, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.2766, Batch Acc: 0.9375\n",
            "Epoch 12 Summary - Loss: 101.6364, Train Accuracy: 0.8543\n",
            "Validation Accuracy: 0.8803\n",
            "\n",
            "Epoch 13/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.8086, Batch Acc: 0.7500\n",
            "[Batch 20/251] Loss: 0.4329, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.2952, Batch Acc: 1.0000\n",
            "[Batch 40/251] Loss: 0.5116, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.2959, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.3079, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.4003, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.3690, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.3492, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.3094, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.2121, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.4521, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.1478, Batch Acc: 1.0000\n",
            "[Batch 140/251] Loss: 0.1927, Batch Acc: 1.0000\n",
            "[Batch 150/251] Loss: 0.5705, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.5407, Batch Acc: 0.7500\n",
            "[Batch 170/251] Loss: 0.1541, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.3089, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.7158, Batch Acc: 0.7500\n",
            "[Batch 200/251] Loss: 0.3818, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.2605, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.5434, Batch Acc: 0.7500\n",
            "[Batch 230/251] Loss: 0.3765, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.4535, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.5450, Batch Acc: 0.8125\n",
            "Epoch 13 Summary - Loss: 101.9817, Train Accuracy: 0.8523\n",
            "Validation Accuracy: 0.8764\n",
            "\n",
            "Epoch 14/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1278, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.5036, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.4486, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.4705, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.4428, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.5531, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.3356, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.4925, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.5122, Batch Acc: 0.7500\n",
            "[Batch 100/251] Loss: 0.5258, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.6998, Batch Acc: 0.6875\n",
            "[Batch 120/251] Loss: 0.5165, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.4228, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.7182, Batch Acc: 0.6875\n",
            "[Batch 150/251] Loss: 0.7247, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.5456, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 1.2396, Batch Acc: 0.5000\n",
            "[Batch 180/251] Loss: 0.8340, Batch Acc: 0.6875\n",
            "[Batch 190/251] Loss: 0.2375, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.5471, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.5038, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.8275, Batch Acc: 0.6250\n",
            "[Batch 230/251] Loss: 0.6983, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 1.4159, Batch Acc: 0.5000\n",
            "[Batch 250/251] Loss: 0.1858, Batch Acc: 0.8750\n",
            "Epoch 14 Summary - Loss: 102.8228, Train Accuracy: 0.8431\n",
            "Validation Accuracy: 0.8684\n",
            "\n",
            "Epoch 15/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2193, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.3612, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.4217, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.2564, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.2980, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.6362, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.4182, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.1888, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.3305, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.1150, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.3665, Batch Acc: 0.7500\n",
            "[Batch 120/251] Loss: 0.4556, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.1667, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.2853, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.5715, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.4879, Batch Acc: 0.7500\n",
            "[Batch 170/251] Loss: 0.4069, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.7053, Batch Acc: 0.5625\n",
            "[Batch 190/251] Loss: 0.1667, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.1605, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.3273, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.7315, Batch Acc: 0.6250\n",
            "[Batch 230/251] Loss: 0.3615, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.3634, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.1957, Batch Acc: 1.0000\n",
            "Epoch 15 Summary - Loss: 102.0895, Train Accuracy: 0.8496\n",
            "Validation Accuracy: 0.8793\n",
            "\n",
            "Epoch 16/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4263, Batch Acc: 0.7500\n",
            "[Batch 20/251] Loss: 0.1738, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 1.0549, Batch Acc: 0.5625\n",
            "[Batch 40/251] Loss: 0.4713, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.5967, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.3406, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.6561, Batch Acc: 0.6875\n",
            "[Batch 80/251] Loss: 0.6082, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.2810, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.6598, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.9028, Batch Acc: 0.7500\n",
            "[Batch 120/251] Loss: 0.3914, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.2262, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.1858, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.4913, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.1918, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.4099, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.7849, Batch Acc: 0.7500\n",
            "[Batch 190/251] Loss: 0.3414, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.2807, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.8149, Batch Acc: 0.7500\n",
            "[Batch 220/251] Loss: 0.2532, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.3621, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.3641, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.5555, Batch Acc: 0.8750\n",
            "Epoch 16 Summary - Loss: 102.1948, Train Accuracy: 0.8553\n",
            "Validation Accuracy: 0.8773\n",
            "\n",
            "Epoch 17/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2739, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.3935, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.3275, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.1360, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.5038, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.8582, Batch Acc: 0.6875\n",
            "[Batch 70/251] Loss: 0.1956, Batch Acc: 1.0000\n",
            "[Batch 80/251] Loss: 0.3473, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.3817, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.2043, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.1083, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.2108, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.3230, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.3749, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.1293, Batch Acc: 1.0000\n",
            "[Batch 160/251] Loss: 0.2265, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.7918, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.5781, Batch Acc: 0.6875\n",
            "[Batch 190/251] Loss: 0.3068, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.6708, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.5834, Batch Acc: 0.7500\n",
            "[Batch 220/251] Loss: 0.4472, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.3834, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.2209, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.4539, Batch Acc: 0.8125\n",
            "Epoch 17 Summary - Loss: 97.9518, Train Accuracy: 0.8563\n",
            "Validation Accuracy: 0.8833\n",
            "\n",
            "Epoch 18/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1784, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.3821, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.7036, Batch Acc: 0.7500\n",
            "[Batch 40/251] Loss: 0.3047, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.6372, Batch Acc: 0.6875\n",
            "[Batch 60/251] Loss: 0.1421, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.3146, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.2585, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.2946, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.3830, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.4949, Batch Acc: 0.7500\n",
            "[Batch 120/251] Loss: 0.5569, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.3571, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.0923, Batch Acc: 1.0000\n",
            "[Batch 150/251] Loss: 0.1383, Batch Acc: 1.0000\n",
            "[Batch 160/251] Loss: 0.3399, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.0625, Batch Acc: 1.0000\n",
            "[Batch 180/251] Loss: 0.4460, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.4804, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.2215, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.5223, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.5047, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.3097, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.2397, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.1541, Batch Acc: 1.0000\n",
            "Epoch 18 Summary - Loss: 96.2442, Train Accuracy: 0.8593\n",
            "Validation Accuracy: 0.8734\n",
            "\n",
            "Epoch 19/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.6256, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.1562, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.5211, Batch Acc: 0.6875\n",
            "[Batch 40/251] Loss: 0.2551, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.2530, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.1763, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.3824, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.1349, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.5737, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.3953, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.1692, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.3556, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.4099, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.1815, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.3231, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.3531, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.3048, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.0672, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.5587, Batch Acc: 0.8125\n",
            "[Batch 200/251] Loss: 0.2944, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.6723, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.3534, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.3826, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.5170, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.6365, Batch Acc: 0.8125\n",
            "Epoch 19 Summary - Loss: 99.5370, Train Accuracy: 0.8555\n",
            "Validation Accuracy: 0.8843\n",
            "\n",
            "Epoch 20/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1733, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.1309, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.7269, Batch Acc: 0.6875\n",
            "[Batch 40/251] Loss: 0.3372, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.4977, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.5068, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.4699, Batch Acc: 0.7500\n",
            "[Batch 80/251] Loss: 0.5260, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.2566, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.2869, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.1926, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.5546, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.3584, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.2785, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.2847, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.4105, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.3610, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.1327, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.3618, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.2014, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.0533, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.4314, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.3610, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.2790, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.6057, Batch Acc: 0.6875\n",
            "Epoch 20 Summary - Loss: 95.0029, Train Accuracy: 0.8623\n",
            "Validation Accuracy: 0.8625\n",
            "\n",
            "Epoch 21/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1424, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.3562, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.4956, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.6297, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.2135, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.2527, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.3611, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.1672, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.3484, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.4161, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.3924, Batch Acc: 0.7500\n",
            "[Batch 120/251] Loss: 0.4264, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.2342, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.5296, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.7093, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.2070, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.7313, Batch Acc: 0.6875\n",
            "[Batch 180/251] Loss: 0.1640, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.3731, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.7340, Batch Acc: 0.6875\n",
            "[Batch 210/251] Loss: 0.2774, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.7953, Batch Acc: 0.7500\n",
            "[Batch 230/251] Loss: 0.1877, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.2415, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.4321, Batch Acc: 0.8125\n",
            "Epoch 21 Summary - Loss: 100.7706, Train Accuracy: 0.8505\n",
            "Validation Accuracy: 0.8714\n",
            "\n",
            "Epoch 22/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.3693, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.1960, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.6890, Batch Acc: 0.6875\n",
            "[Batch 40/251] Loss: 0.3999, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.6077, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.4219, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.2676, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.4239, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.3842, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.5511, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.3634, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.2444, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.5473, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.2293, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.3320, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.0562, Batch Acc: 1.0000\n",
            "[Batch 170/251] Loss: 0.5383, Batch Acc: 0.7500\n",
            "[Batch 180/251] Loss: 0.3357, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.2176, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.6060, Batch Acc: 0.6875\n",
            "[Batch 210/251] Loss: 0.2599, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.3306, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.3030, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.3814, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.3164, Batch Acc: 0.8750\n",
            "Epoch 22 Summary - Loss: 95.3027, Train Accuracy: 0.8590\n",
            "Validation Accuracy: 0.8754\n",
            "\n",
            "Epoch 23/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.5798, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.7792, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.3859, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 1.0128, Batch Acc: 0.6875\n",
            "[Batch 50/251] Loss: 0.5235, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.7454, Batch Acc: 0.6875\n",
            "[Batch 70/251] Loss: 0.2381, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.2004, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.2221, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.2431, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.3181, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.1710, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.2285, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.7941, Batch Acc: 0.6875\n",
            "[Batch 150/251] Loss: 0.3510, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.9751, Batch Acc: 0.6875\n",
            "[Batch 170/251] Loss: 0.2954, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.3282, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.3506, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.2558, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.2149, Batch Acc: 1.0000\n",
            "[Batch 220/251] Loss: 0.3028, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.2453, Batch Acc: 1.0000\n",
            "[Batch 240/251] Loss: 0.2096, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.1359, Batch Acc: 1.0000\n",
            "Epoch 23 Summary - Loss: 97.4606, Train Accuracy: 0.8600\n",
            "Validation Accuracy: 0.8783\n",
            "\n",
            "Epoch 24/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.8192, Batch Acc: 0.7500\n",
            "[Batch 20/251] Loss: 0.1748, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.4005, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.5240, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.3940, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.4372, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.2792, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.4999, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.0839, Batch Acc: 1.0000\n",
            "[Batch 100/251] Loss: 0.0753, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.4070, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.5415, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.3669, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.3713, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.5174, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.2173, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.5184, Batch Acc: 0.7500\n",
            "[Batch 180/251] Loss: 0.3079, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.4715, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.4453, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.4678, Batch Acc: 0.7500\n",
            "[Batch 220/251] Loss: 0.4955, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.3938, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.1141, Batch Acc: 1.0000\n",
            "[Batch 250/251] Loss: 0.1264, Batch Acc: 0.9375\n",
            "Epoch 24 Summary - Loss: 100.1468, Train Accuracy: 0.8483\n",
            "Validation Accuracy: 0.8526\n",
            "\n",
            "Epoch 25/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2406, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.1916, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.2917, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.1618, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.2120, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.4820, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.6418, Batch Acc: 0.6250\n",
            "[Batch 80/251] Loss: 0.3417, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.1746, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.5623, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.1371, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.6027, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.9778, Batch Acc: 0.5625\n",
            "[Batch 140/251] Loss: 0.1073, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.2636, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.1652, Batch Acc: 1.0000\n",
            "[Batch 170/251] Loss: 0.3797, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.6169, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.2655, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.2361, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.6363, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.1183, Batch Acc: 1.0000\n",
            "[Batch 230/251] Loss: 0.1021, Batch Acc: 1.0000\n",
            "[Batch 240/251] Loss: 0.1648, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.5700, Batch Acc: 0.8125\n",
            "Epoch 25 Summary - Loss: 95.6480, Train Accuracy: 0.8640\n",
            "Validation Accuracy: 0.8764\n",
            "\n",
            "Epoch 26/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.5617, Batch Acc: 0.6875\n",
            "[Batch 20/251] Loss: 0.1213, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.3640, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.4955, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.7453, Batch Acc: 0.6250\n",
            "[Batch 60/251] Loss: 0.6881, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.6450, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.4283, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.4525, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.5242, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.4050, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.3976, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.2578, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.2076, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.5872, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.3346, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.3111, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.2094, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.2326, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.0708, Batch Acc: 1.0000\n",
            "[Batch 210/251] Loss: 0.2383, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.2972, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.3433, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.5969, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.2900, Batch Acc: 0.8125\n",
            "Epoch 26 Summary - Loss: 102.1267, Train Accuracy: 0.8538\n",
            "Validation Accuracy: 0.8754\n",
            "\n",
            "Epoch 27/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1811, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.7436, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.2675, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.2946, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.6286, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.5151, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.3541, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.5081, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.4989, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.1931, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.4809, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.2860, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.6578, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.3936, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.5949, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.2453, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.2376, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.6964, Batch Acc: 0.6250\n",
            "[Batch 190/251] Loss: 0.2274, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.1214, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.3058, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.3119, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.8642, Batch Acc: 0.6875\n",
            "[Batch 240/251] Loss: 0.4969, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.1071, Batch Acc: 1.0000\n",
            "Epoch 27 Summary - Loss: 95.8683, Train Accuracy: 0.8550\n",
            "Validation Accuracy: 0.8744\n",
            "\n",
            "Epoch 28/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4824, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.4721, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.5562, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.2364, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.8020, Batch Acc: 0.6250\n",
            "[Batch 60/251] Loss: 0.8725, Batch Acc: 0.6875\n",
            "[Batch 70/251] Loss: 0.4355, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.3389, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.2797, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.2279, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.5104, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.1794, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.4278, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.4418, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.9623, Batch Acc: 0.6875\n",
            "[Batch 160/251] Loss: 0.5033, Batch Acc: 0.7500\n",
            "[Batch 170/251] Loss: 0.1733, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.5432, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.3553, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.2220, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.5257, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.4012, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.3268, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.3310, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.1882, Batch Acc: 0.8750\n",
            "Epoch 28 Summary - Loss: 96.8259, Train Accuracy: 0.8613\n",
            "Validation Accuracy: 0.8675\n",
            "\n",
            "Epoch 29/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1813, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.5778, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.5191, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.2285, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.1127, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.2847, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.6962, Batch Acc: 0.7500\n",
            "[Batch 80/251] Loss: 0.3329, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.3296, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.6905, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.2373, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.1574, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.3812, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.1578, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.7328, Batch Acc: 0.6875\n",
            "[Batch 160/251] Loss: 0.2778, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.4313, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.3429, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.9006, Batch Acc: 0.6875\n",
            "[Batch 200/251] Loss: 0.1904, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.4673, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.4588, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.1687, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.2156, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.2668, Batch Acc: 0.9375\n",
            "Epoch 29 Summary - Loss: 93.4685, Train Accuracy: 0.8615\n",
            "Validation Accuracy: 0.8605\n",
            "\n",
            "Epoch 30/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.4745, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.2557, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.7178, Batch Acc: 0.7500\n",
            "[Batch 40/251] Loss: 0.1918, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.5925, Batch Acc: 0.6875\n",
            "[Batch 60/251] Loss: 0.2131, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.2290, Batch Acc: 1.0000\n",
            "[Batch 80/251] Loss: 0.2066, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.5698, Batch Acc: 0.7500\n",
            "[Batch 100/251] Loss: 0.1516, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.2233, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.6226, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.1872, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.1039, Batch Acc: 1.0000\n",
            "[Batch 150/251] Loss: 0.2849, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.1816, Batch Acc: 1.0000\n",
            "[Batch 170/251] Loss: 0.4008, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.2400, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.2832, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 1.1475, Batch Acc: 0.5000\n",
            "[Batch 210/251] Loss: 0.3886, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.4486, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.2916, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.5308, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.0635, Batch Acc: 1.0000\n",
            "Epoch 30 Summary - Loss: 89.6880, Train Accuracy: 0.8660\n",
            "Validation Accuracy: 0.8675\n",
            "\n",
            "Epoch 31/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 1.0040, Batch Acc: 0.6250\n",
            "[Batch 20/251] Loss: 0.2661, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.7971, Batch Acc: 0.7500\n",
            "[Batch 40/251] Loss: 0.1200, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.1834, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.5981, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.3863, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.6208, Batch Acc: 0.6875\n",
            "[Batch 90/251] Loss: 0.3315, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.3174, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.0799, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.2517, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.1987, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.5674, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.2518, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.2198, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.4790, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.2091, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.3887, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.5567, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.3282, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.3778, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.9837, Batch Acc: 0.6875\n",
            "[Batch 240/251] Loss: 0.4946, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.4680, Batch Acc: 0.7500\n",
            "Epoch 31 Summary - Loss: 96.6023, Train Accuracy: 0.8538\n",
            "Validation Accuracy: 0.8754\n",
            "\n",
            "Epoch 32/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2224, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.5280, Batch Acc: 0.6875\n",
            "[Batch 30/251] Loss: 0.7336, Batch Acc: 0.6250\n",
            "[Batch 40/251] Loss: 0.5692, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.1258, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.1673, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.2422, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.3593, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.5612, Batch Acc: 0.6875\n",
            "[Batch 100/251] Loss: 0.4290, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.2803, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.3292, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.5032, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.5512, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.1461, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.9984, Batch Acc: 0.6875\n",
            "[Batch 170/251] Loss: 0.3080, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.1994, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.1595, Batch Acc: 1.0000\n",
            "[Batch 200/251] Loss: 0.3566, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.7620, Batch Acc: 0.7500\n",
            "[Batch 220/251] Loss: 0.2735, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.3059, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.3006, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.1725, Batch Acc: 1.0000\n",
            "Epoch 32 Summary - Loss: 93.5316, Train Accuracy: 0.8633\n",
            "Validation Accuracy: 0.8773\n",
            "\n",
            "Epoch 33/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2681, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.6699, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.3227, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.1789, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.3722, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.1932, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.4274, Batch Acc: 0.7500\n",
            "[Batch 80/251] Loss: 0.3472, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.1200, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.4738, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.5254, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.4888, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.3084, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.4128, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.2987, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.3542, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.6139, Batch Acc: 0.7500\n",
            "[Batch 180/251] Loss: 0.6833, Batch Acc: 0.6875\n",
            "[Batch 190/251] Loss: 0.1744, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.6393, Batch Acc: 0.6875\n",
            "[Batch 210/251] Loss: 0.5392, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.2915, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.3272, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 1.0275, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.6109, Batch Acc: 0.7500\n",
            "Epoch 33 Summary - Loss: 99.3344, Train Accuracy: 0.8515\n",
            "Validation Accuracy: 0.8724\n",
            "\n",
            "Epoch 34/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.0917, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.1048, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.1787, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.7395, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.2188, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.2224, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.6204, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.4240, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 1.0829, Batch Acc: 0.5625\n",
            "[Batch 100/251] Loss: 0.2350, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.5871, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.2380, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.4331, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.4245, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.1938, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.5670, Batch Acc: 0.7500\n",
            "[Batch 170/251] Loss: 0.2559, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.1899, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.2408, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.0926, Batch Acc: 1.0000\n",
            "[Batch 210/251] Loss: 0.3234, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.7504, Batch Acc: 0.6875\n",
            "[Batch 230/251] Loss: 0.1969, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.2867, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.5854, Batch Acc: 0.7500\n",
            "Epoch 34 Summary - Loss: 91.6022, Train Accuracy: 0.8678\n",
            "Validation Accuracy: 0.8793\n",
            "\n",
            "Epoch 35/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.3442, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.8854, Batch Acc: 0.6250\n",
            "[Batch 30/251] Loss: 0.4498, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.3895, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.7672, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.1868, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.2813, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.4788, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.2672, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.6805, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.1779, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.1447, Batch Acc: 1.0000\n",
            "[Batch 130/251] Loss: 0.7751, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.5113, Batch Acc: 0.7500\n",
            "[Batch 150/251] Loss: 0.1829, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.1979, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.3454, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.2106, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.3706, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.1068, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.1554, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.1266, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.4963, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.3672, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.9315, Batch Acc: 0.6875\n",
            "Epoch 35 Summary - Loss: 94.6271, Train Accuracy: 0.8623\n",
            "Validation Accuracy: 0.8803\n",
            "\n",
            "Epoch 36/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2130, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.2363, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.5242, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.3260, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.5775, Batch Acc: 0.6875\n",
            "[Batch 60/251] Loss: 0.1722, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.2548, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.1906, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.1914, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.0820, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.5985, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.1977, Batch Acc: 1.0000\n",
            "[Batch 130/251] Loss: 0.2254, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.2979, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.3302, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.6968, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.2482, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.3611, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.2378, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.1681, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.2124, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.9438, Batch Acc: 0.6875\n",
            "[Batch 230/251] Loss: 0.1162, Batch Acc: 1.0000\n",
            "[Batch 240/251] Loss: 0.3350, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.3308, Batch Acc: 0.7500\n",
            "Epoch 36 Summary - Loss: 94.6038, Train Accuracy: 0.8590\n",
            "Validation Accuracy: 0.8754\n",
            "\n",
            "Epoch 37/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2596, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.3781, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.1943, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.2655, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.1798, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.0627, Batch Acc: 1.0000\n",
            "[Batch 70/251] Loss: 0.0849, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.1682, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.1194, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.5244, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.6277, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.4310, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.4891, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.3199, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.1746, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.4604, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.1715, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.1980, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.2577, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.5329, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.2977, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.1457, Batch Acc: 1.0000\n",
            "[Batch 230/251] Loss: 0.1207, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.3914, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.3288, Batch Acc: 0.8750\n",
            "Epoch 37 Summary - Loss: 94.8657, Train Accuracy: 0.8645\n",
            "Validation Accuracy: 0.8714\n",
            "\n",
            "Epoch 38/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.3706, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.2325, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.2537, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.5075, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.2662, Batch Acc: 0.8750\n",
            "[Batch 60/251] Loss: 0.2590, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.5509, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.0551, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.4545, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.1070, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.1082, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.5254, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 1.0441, Batch Acc: 0.6875\n",
            "[Batch 140/251] Loss: 0.1612, Batch Acc: 1.0000\n",
            "[Batch 150/251] Loss: 0.5925, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.0992, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.3178, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.1409, Batch Acc: 1.0000\n",
            "[Batch 190/251] Loss: 0.3221, Batch Acc: 0.8125\n",
            "[Batch 200/251] Loss: 0.4930, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.5347, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.2126, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.3298, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.1756, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.0766, Batch Acc: 1.0000\n",
            "Epoch 38 Summary - Loss: 94.5623, Train Accuracy: 0.8640\n",
            "Validation Accuracy: 0.8694\n",
            "\n",
            "Epoch 39/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2941, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.1562, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.2856, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.2657, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.1986, Batch Acc: 1.0000\n",
            "[Batch 60/251] Loss: 0.5063, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.0994, Batch Acc: 1.0000\n",
            "[Batch 80/251] Loss: 0.2582, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.4479, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.3697, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.3064, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.7124, Batch Acc: 0.6875\n",
            "[Batch 130/251] Loss: 0.3318, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.2470, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.3059, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.2507, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.6903, Batch Acc: 0.7500\n",
            "[Batch 180/251] Loss: 0.2476, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.2434, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.4038, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.3846, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.2660, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.4937, Batch Acc: 0.7500\n",
            "[Batch 240/251] Loss: 0.3959, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.0851, Batch Acc: 1.0000\n",
            "Epoch 39 Summary - Loss: 90.0187, Train Accuracy: 0.8590\n",
            "Validation Accuracy: 0.8684\n",
            "\n",
            "Epoch 40/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.6514, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.6572, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.2643, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.1632, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.2822, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.3569, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.4226, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.1890, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.5728, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.3093, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.1343, Batch Acc: 1.0000\n",
            "[Batch 120/251] Loss: 0.1784, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.9396, Batch Acc: 0.6875\n",
            "[Batch 140/251] Loss: 0.3115, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.4773, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.2142, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.2986, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.3014, Batch Acc: 0.8125\n",
            "[Batch 190/251] Loss: 0.4902, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.3318, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.2026, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.1797, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.2751, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.7004, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.1853, Batch Acc: 0.9375\n",
            "Epoch 40 Summary - Loss: 92.7964, Train Accuracy: 0.8605\n",
            "Validation Accuracy: 0.8615\n",
            "\n",
            "Epoch 41/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2539, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.1646, Batch Acc: 0.9375\n",
            "[Batch 30/251] Loss: 0.1707, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.4099, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.7888, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.3840, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.1126, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.3630, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.2556, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.0800, Batch Acc: 1.0000\n",
            "[Batch 110/251] Loss: 0.5764, Batch Acc: 0.7500\n",
            "[Batch 120/251] Loss: 0.2863, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.2837, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.7846, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.3789, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.1598, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.1002, Batch Acc: 1.0000\n",
            "[Batch 180/251] Loss: 0.7254, Batch Acc: 0.6875\n",
            "[Batch 190/251] Loss: 0.9411, Batch Acc: 0.6875\n",
            "[Batch 200/251] Loss: 0.3595, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.3926, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.5921, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.3666, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.2100, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.0940, Batch Acc: 1.0000\n",
            "Epoch 41 Summary - Loss: 93.8543, Train Accuracy: 0.8603\n",
            "Validation Accuracy: 0.8754\n",
            "\n",
            "Epoch 42/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.3705, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.2452, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.1663, Batch Acc: 1.0000\n",
            "[Batch 40/251] Loss: 0.3842, Batch Acc: 0.8125\n",
            "[Batch 50/251] Loss: 0.6131, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.4963, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.3867, Batch Acc: 0.8125\n",
            "[Batch 80/251] Loss: 0.0900, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.7799, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.1585, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.3106, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.4853, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.7541, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.3527, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.1272, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.2624, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.3176, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.4657, Batch Acc: 0.7500\n",
            "[Batch 190/251] Loss: 0.0857, Batch Acc: 1.0000\n",
            "[Batch 200/251] Loss: 0.2372, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.3775, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.1872, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.5226, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.4118, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.2816, Batch Acc: 0.8750\n",
            "Epoch 42 Summary - Loss: 93.1986, Train Accuracy: 0.8643\n",
            "Validation Accuracy: 0.8793\n",
            "\n",
            "Epoch 43/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.3942, Batch Acc: 0.8125\n",
            "[Batch 20/251] Loss: 0.5756, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.4252, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.5570, Batch Acc: 0.7500\n",
            "[Batch 50/251] Loss: 0.5767, Batch Acc: 0.6875\n",
            "[Batch 60/251] Loss: 0.3899, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.3208, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.1855, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 0.3075, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.5682, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.5085, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.3101, Batch Acc: 0.8125\n",
            "[Batch 130/251] Loss: 0.6709, Batch Acc: 0.7500\n",
            "[Batch 140/251] Loss: 0.5788, Batch Acc: 0.8125\n",
            "[Batch 150/251] Loss: 0.3769, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.2346, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.2567, Batch Acc: 0.8750\n",
            "[Batch 180/251] Loss: 0.4969, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.3149, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.3001, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.1822, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.1056, Batch Acc: 1.0000\n",
            "[Batch 230/251] Loss: 0.2827, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.7918, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.3534, Batch Acc: 0.8750\n",
            "Epoch 43 Summary - Loss: 97.1702, Train Accuracy: 0.8643\n",
            "Validation Accuracy: 0.8783\n",
            "\n",
            "Epoch 44/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.9408, Batch Acc: 0.6250\n",
            "[Batch 20/251] Loss: 0.4258, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.4000, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.2088, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.6739, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.3729, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.4488, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.1061, Batch Acc: 1.0000\n",
            "[Batch 90/251] Loss: 1.1752, Batch Acc: 0.5625\n",
            "[Batch 100/251] Loss: 0.5506, Batch Acc: 0.7500\n",
            "[Batch 110/251] Loss: 0.5398, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.7498, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.2786, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.1828, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.5828, Batch Acc: 0.8750\n",
            "[Batch 160/251] Loss: 0.5926, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.2661, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.1707, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.3645, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.6852, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.4327, Batch Acc: 0.7500\n",
            "[Batch 220/251] Loss: 0.6808, Batch Acc: 0.7500\n",
            "[Batch 230/251] Loss: 0.2199, Batch Acc: 0.9375\n",
            "[Batch 240/251] Loss: 0.4256, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.1854, Batch Acc: 0.9375\n",
            "Epoch 44 Summary - Loss: 101.1006, Train Accuracy: 0.8560\n",
            "Validation Accuracy: 0.8803\n",
            "\n",
            "Epoch 45/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.2902, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.5853, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.2903, Batch Acc: 0.9375\n",
            "[Batch 40/251] Loss: 0.4376, Batch Acc: 0.8750\n",
            "[Batch 50/251] Loss: 0.7624, Batch Acc: 0.7500\n",
            "[Batch 60/251] Loss: 0.6090, Batch Acc: 0.8125\n",
            "[Batch 70/251] Loss: 0.2414, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.2823, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.3299, Batch Acc: 0.8125\n",
            "[Batch 100/251] Loss: 0.3812, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.1383, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.2226, Batch Acc: 0.9375\n",
            "[Batch 130/251] Loss: 0.5185, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.1962, Batch Acc: 1.0000\n",
            "[Batch 150/251] Loss: 0.1995, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.1380, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.6748, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.1633, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.5794, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.2951, Batch Acc: 0.8750\n",
            "[Batch 210/251] Loss: 0.2782, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.2289, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.5537, Batch Acc: 0.8125\n",
            "[Batch 240/251] Loss: 0.3174, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.3151, Batch Acc: 0.8750\n",
            "Epoch 45 Summary - Loss: 95.4696, Train Accuracy: 0.8583\n",
            "Validation Accuracy: 0.8773\n",
            "\n",
            "Epoch 46/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1261, Batch Acc: 0.9375\n",
            "[Batch 20/251] Loss: 0.4828, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.2803, Batch Acc: 0.8750\n",
            "[Batch 40/251] Loss: 0.1789, Batch Acc: 1.0000\n",
            "[Batch 50/251] Loss: 0.5300, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.7811, Batch Acc: 0.6250\n",
            "[Batch 70/251] Loss: 0.2202, Batch Acc: 0.9375\n",
            "[Batch 80/251] Loss: 0.2951, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.2287, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.3514, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.2851, Batch Acc: 0.8125\n",
            "[Batch 120/251] Loss: 0.6102, Batch Acc: 0.7500\n",
            "[Batch 130/251] Loss: 0.3322, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.1108, Batch Acc: 1.0000\n",
            "[Batch 150/251] Loss: 0.9115, Batch Acc: 0.6875\n",
            "[Batch 160/251] Loss: 1.2822, Batch Acc: 0.6875\n",
            "[Batch 170/251] Loss: 0.1782, Batch Acc: 1.0000\n",
            "[Batch 180/251] Loss: 0.2597, Batch Acc: 0.8750\n",
            "[Batch 190/251] Loss: 0.2481, Batch Acc: 0.9375\n",
            "[Batch 200/251] Loss: 0.3512, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.0844, Batch Acc: 0.9375\n",
            "[Batch 220/251] Loss: 0.4686, Batch Acc: 0.6875\n",
            "[Batch 230/251] Loss: 0.3595, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.2875, Batch Acc: 0.7500\n",
            "[Batch 250/251] Loss: 0.6179, Batch Acc: 0.8125\n",
            "Epoch 46 Summary - Loss: 90.2400, Train Accuracy: 0.8610\n",
            "Validation Accuracy: 0.8863\n",
            "\n",
            "Epoch 47/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.1166, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.3250, Batch Acc: 0.8750\n",
            "[Batch 30/251] Loss: 0.1016, Batch Acc: 1.0000\n",
            "[Batch 40/251] Loss: 0.3624, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.2518, Batch Acc: 0.9375\n",
            "[Batch 60/251] Loss: 0.3069, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.3161, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.4431, Batch Acc: 0.8750\n",
            "[Batch 90/251] Loss: 0.2935, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.3338, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.2991, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.0890, Batch Acc: 1.0000\n",
            "[Batch 130/251] Loss: 0.2313, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.4825, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.6248, Batch Acc: 0.7500\n",
            "[Batch 160/251] Loss: 0.4410, Batch Acc: 0.8125\n",
            "[Batch 170/251] Loss: 0.5865, Batch Acc: 0.7500\n",
            "[Batch 180/251] Loss: 0.1119, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.3466, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.5603, Batch Acc: 0.8125\n",
            "[Batch 210/251] Loss: 0.3104, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.4621, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.4600, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.3335, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.2512, Batch Acc: 0.8125\n",
            "Epoch 47 Summary - Loss: 90.5372, Train Accuracy: 0.8670\n",
            "Validation Accuracy: 0.8773\n",
            "\n",
            "Epoch 48/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.0710, Batch Acc: 1.0000\n",
            "[Batch 20/251] Loss: 0.0477, Batch Acc: 1.0000\n",
            "[Batch 30/251] Loss: 0.4226, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.1554, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.2247, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.1873, Batch Acc: 0.8750\n",
            "[Batch 70/251] Loss: 0.4978, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.4273, Batch Acc: 0.7500\n",
            "[Batch 90/251] Loss: 0.2542, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.6163, Batch Acc: 0.8125\n",
            "[Batch 110/251] Loss: 0.2776, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.9755, Batch Acc: 0.6875\n",
            "[Batch 130/251] Loss: 0.1445, Batch Acc: 0.9375\n",
            "[Batch 140/251] Loss: 0.3356, Batch Acc: 0.9375\n",
            "[Batch 150/251] Loss: 0.5864, Batch Acc: 0.8125\n",
            "[Batch 160/251] Loss: 0.2142, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.2723, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.2425, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.3529, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.7168, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.1743, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.2525, Batch Acc: 0.9375\n",
            "[Batch 230/251] Loss: 0.3587, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.2544, Batch Acc: 0.9375\n",
            "[Batch 250/251] Loss: 0.4572, Batch Acc: 0.7500\n",
            "Epoch 48 Summary - Loss: 89.2432, Train Accuracy: 0.8683\n",
            "Validation Accuracy: 0.8833\n",
            "\n",
            "Epoch 49/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 0.3645, Batch Acc: 0.8750\n",
            "[Batch 20/251] Loss: 0.3555, Batch Acc: 0.8125\n",
            "[Batch 30/251] Loss: 0.4332, Batch Acc: 0.8125\n",
            "[Batch 40/251] Loss: 0.1559, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.6243, Batch Acc: 0.6250\n",
            "[Batch 60/251] Loss: 0.6111, Batch Acc: 0.7500\n",
            "[Batch 70/251] Loss: 0.8737, Batch Acc: 0.6250\n",
            "[Batch 80/251] Loss: 0.0837, Batch Acc: 0.9375\n",
            "[Batch 90/251] Loss: 0.1822, Batch Acc: 0.9375\n",
            "[Batch 100/251] Loss: 0.2365, Batch Acc: 0.8750\n",
            "[Batch 110/251] Loss: 0.3020, Batch Acc: 0.9375\n",
            "[Batch 120/251] Loss: 0.4178, Batch Acc: 0.8750\n",
            "[Batch 130/251] Loss: 0.6393, Batch Acc: 0.8125\n",
            "[Batch 140/251] Loss: 0.1929, Batch Acc: 1.0000\n",
            "[Batch 150/251] Loss: 0.2977, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.1781, Batch Acc: 0.9375\n",
            "[Batch 170/251] Loss: 0.2542, Batch Acc: 0.9375\n",
            "[Batch 180/251] Loss: 0.0951, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.0517, Batch Acc: 1.0000\n",
            "[Batch 200/251] Loss: 0.7508, Batch Acc: 0.7500\n",
            "[Batch 210/251] Loss: 0.3348, Batch Acc: 0.8750\n",
            "[Batch 220/251] Loss: 0.3029, Batch Acc: 0.8125\n",
            "[Batch 230/251] Loss: 0.3856, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.3589, Batch Acc: 0.8750\n",
            "[Batch 250/251] Loss: 0.2047, Batch Acc: 0.8750\n",
            "Epoch 49 Summary - Loss: 94.4233, Train Accuracy: 0.8545\n",
            "Validation Accuracy: 0.8843\n",
            "\n",
            "Epoch 50/50\n",
            "------------------------------\n",
            "[Batch 10/251] Loss: 1.2072, Batch Acc: 0.6250\n",
            "[Batch 20/251] Loss: 0.7680, Batch Acc: 0.7500\n",
            "[Batch 30/251] Loss: 0.0412, Batch Acc: 1.0000\n",
            "[Batch 40/251] Loss: 0.2846, Batch Acc: 0.9375\n",
            "[Batch 50/251] Loss: 0.2917, Batch Acc: 0.8125\n",
            "[Batch 60/251] Loss: 0.2355, Batch Acc: 0.9375\n",
            "[Batch 70/251] Loss: 0.2900, Batch Acc: 0.8750\n",
            "[Batch 80/251] Loss: 0.4286, Batch Acc: 0.8125\n",
            "[Batch 90/251] Loss: 0.8138, Batch Acc: 0.8750\n",
            "[Batch 100/251] Loss: 0.2765, Batch Acc: 0.9375\n",
            "[Batch 110/251] Loss: 0.3686, Batch Acc: 0.8750\n",
            "[Batch 120/251] Loss: 0.1194, Batch Acc: 1.0000\n",
            "[Batch 130/251] Loss: 0.3986, Batch Acc: 0.8750\n",
            "[Batch 140/251] Loss: 0.2690, Batch Acc: 0.8750\n",
            "[Batch 150/251] Loss: 0.3888, Batch Acc: 0.9375\n",
            "[Batch 160/251] Loss: 0.3632, Batch Acc: 0.8750\n",
            "[Batch 170/251] Loss: 0.5139, Batch Acc: 0.8125\n",
            "[Batch 180/251] Loss: 0.1527, Batch Acc: 0.9375\n",
            "[Batch 190/251] Loss: 0.4552, Batch Acc: 0.8750\n",
            "[Batch 200/251] Loss: 0.2849, Batch Acc: 0.9375\n",
            "[Batch 210/251] Loss: 0.7024, Batch Acc: 0.8125\n",
            "[Batch 220/251] Loss: 0.4132, Batch Acc: 0.8750\n",
            "[Batch 230/251] Loss: 0.5448, Batch Acc: 0.8750\n",
            "[Batch 240/251] Loss: 0.4341, Batch Acc: 0.8125\n",
            "[Batch 250/251] Loss: 0.2731, Batch Acc: 0.9375\n",
            "Epoch 50 Summary - Loss: 93.7185, Train Accuracy: 0.8653\n",
            "Validation Accuracy: 0.8764\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}